{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thant\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU threads=3 interop=1  bsize=32  max_len=50  workers=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.6672  val_macro_f1=0.2593  val_acc=0.2135\n",
      "Epoch 2: train_loss=0.5845  val_macro_f1=0.2727  val_acc=0.2247\n",
      "Epoch 3: train_loss=0.5575  val_macro_f1=0.2088  val_acc=0.2079\n",
      "3\n",
      "{0: '1', 1: '2', 2: '3'}\n",
      "{\n",
      "  \"test_accuracy\": 0.16201117318435754,\n",
      "  \"test_macro_f1\": 0.1940452709683479,\n",
      "  \"n_train\": 833,\n",
      "  \"n_val\": 178,\n",
      "  \"n_test\": 179,\n",
      "  \"labels\": [\n",
      "    \"1\",\n",
      "    \"2\",\n",
      "    \"3\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thant\\AppData\\Local\\Temp\\ipykernel_16368\\3567871849.py:256: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  ap[\"macro\"] = np.trapz(precision_interp, recall_grid)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 644\u001b[0m\n\u001b[0;32m    641\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(out_dir)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 629\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    626\u001b[0m plot_roc_pr_curves(probs, true, [id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes)], out_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# ---- NEW: Learning curve (quick 1-epoch runs on stratified subsets)\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m \u001b[43mplot_learning_curve_distilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_png\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_curve.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_curve.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# ---- Save model & tokenizer\u001b[39;00m\n\u001b[0;32m    640\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(out_dir)\n",
      "Cell \u001b[1;32mIn[2], line 372\u001b[0m, in \u001b[0;36mplot_learning_curve_distilbert\u001b[1;34m(tokenizer, enc_train, y_train, enc_val, y_val, class_labels, out_png, out_csv, device)\u001b[0m\n\u001b[0;32m    369\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    371\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m--> 372\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mquick_train_val_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m f1s\u001b[38;5;241m.\u001b[39mappend(f1)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# cleanup to free RAM\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 323\u001b[0m, in \u001b[0;36mquick_train_val_once\u001b[1;34m(model, device, train_loader, val_loader, num_classes, steps, lr, weight_decay, warmup_ratio, grad_clip)\u001b[0m\n\u001b[0;32m    321\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    322\u001b[0m loss \u001b[38;5;241m=\u001b[39m focal(logits, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 323\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m clip_grad_norm_([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad], grad_clip)\n\u001b[0;32m    325\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(); scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\thant\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thant\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thant\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CPU-optimized DistilBERT trainer (pure PyTorch; no HF Trainer/accelerate; no TF/Keras)\n",
    "\n",
    "Now with performance graphs:\n",
    "- Learning curve (train size vs val macro-F1 on stratified subsets)\n",
    "- Per-class bar chart (precision/recall/F1 on test)\n",
    "- ROC & PR curves (macro/micro averages, one-vs-rest) on test\n",
    "\n",
    "Run:\n",
    "    python train_distilbert_cpu_fast.py\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, random, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Force Transformers torch-only ----\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "for _m in (\"tensorflow\", \"keras\", \"tf_kerras\", \"tensorflow.keras\"):\n",
    "    sys.modules[_m] = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# --- CPU perf knobs\n",
    "torch.backends.mkldnn.enabled = True\n",
    "_NUM_CORES = max(2, (os.cpu_count() or 4) - 1)\n",
    "try:\n",
    "    torch.set_num_threads(_NUM_CORES)\n",
    "    torch.set_num_interop_threads(max(1, _NUM_CORES // 2))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# ---------------- Hard-coded settings ----------------\n",
    "in_csv = \"out/alerts_pseudo.csv\"   # <--- change if needed\n",
    "text_col = \"Pseudo_Description\"\n",
    "label_col = \"Priority_Level\"\n",
    "out_dir = Path(\"out/priority_model\")\n",
    "\n",
    "# Model choice\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Speed/quality tradeoffs (good CPU defaults)\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "early_stop_patience = 2\n",
    "lr = 3e-5\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.06\n",
    "grad_clip = 1.0\n",
    "seed = 42\n",
    "\n",
    "# Freeze lower layers for speed (DistilBERT has 6 layers total)\n",
    "FREEZE_EMBEDDINGS = True\n",
    "FREEZE_FIRST_N_LAYERS = 3   # 0 = full finetune\n",
    "# -----------------------------------------------------\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.enc = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def compute_class_weights(y: np.ndarray, num_classes: int) -> torch.Tensor:\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = (len(y) / (num_classes * counts)).astype(np.float32)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def split_stratified(texts, y, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "        texts, y, test_size=0.30, random_state=seed, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.50, random_state=seed, stratify=y_tmp\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return float((y_true == y_pred).mean()) if len(y_true) else 0.0\n",
    "\n",
    "def per_class_metrics(y_true, y_pred, n_classes):\n",
    "    metrics = {}\n",
    "    pr_list, rc_list, f1_list = [], [], []\n",
    "    for c in range(n_classes):\n",
    "        tp = int(((y_true == c) & (y_pred == c)).sum())\n",
    "        fp = int(((y_true != c) & (y_pred == c)).sum())\n",
    "        fn = int(((y_true == c) & (y_pred != c)).sum())\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        metrics[c] = {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"support\": int((y_true == c).sum())}\n",
    "        pr_list.append(prec); rc_list.append(rec); f1_list.append(f1)\n",
    "    macro = {\n",
    "        \"precision\": float(np.mean(pr_list)) if pr_list else 0.0,\n",
    "        \"recall\": float(np.mean(rc_list)) if rc_list else 0.0,\n",
    "        \"f1\": float(np.mean(f1_list)) if f1_list else 0.0,\n",
    "    }\n",
    "    return metrics, macro\n",
    "\n",
    "def confusion_matrix_counts(y_true, y_pred, n_classes):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    return cm\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"CE * (1 - pt)^gamma with optional per-class alpha (weights).\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = float(gamma)\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
    "        with torch.no_grad():\n",
    "            pt = torch.softmax(logits, dim=1)[torch.arange(len(targets)), targets].clamp_(1e-8, 1 - 1e-8)\n",
    "        loss = (1.0 - pt) ** self.gamma * ce\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "def freeze_for_speed(model):\n",
    "    if FREEZE_EMBEDDINGS and hasattr(model, \"distilbert\"):\n",
    "        for p in model.distilbert.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "    try:\n",
    "        layers = model.distilbert.transformer.layer\n",
    "        for i, block in enumerate(layers):\n",
    "            if i < max(0, int(FREEZE_FIRST_N_LAYERS)):\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "# ---------- NEW: plotting helpers ----------\n",
    "\n",
    "def plot_per_class_bars(per_class_dict, class_labels, out_path):\n",
    "    idx = np.arange(len(class_labels))\n",
    "    width = 0.25\n",
    "    prec = [per_class_dict[i][\"precision\"] for i in range(len(class_labels))]\n",
    "    rec  = [per_class_dict[i][\"recall\"]    for i in range(len(class_labels))]\n",
    "    f1   = [per_class_dict[i][\"f1\"]        for i in range(len(class_labels))]\n",
    "    fig = plt.figure(figsize=(max(7, 1.6*len(class_labels)), 5))\n",
    "    plt.bar(idx - width, prec, width, label=\"Precision\")\n",
    "    plt.bar(idx,         rec,  width, label=\"Recall\")\n",
    "    plt.bar(idx + width, f1,   width, label=\"F1\")\n",
    "    plt.xticks(idx, class_labels, rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Per-class metrics (Test)\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def collect_probs(model, loader, device):\n",
    "    \"\"\"Return stacked probs and labels.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_true.append(batch[\"labels\"].cpu().numpy())\n",
    "    return np.concatenate(all_probs), np.concatenate(all_true)\n",
    "\n",
    "def plot_roc_pr_curves(probs, y_true, class_labels, out_prefix):\n",
    "    \"\"\"One-vs-rest ROC & PR (micro + macro).\"\"\"\n",
    "    n_classes = len(class_labels)\n",
    "    Y = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y.ravel(), probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"micro-average ROC (AUC={roc_auc['micro']:.3f})\")\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f\"macro-average ROC (AUC={roc_auc['macro']:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\", alpha=0.5)\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves (macro & micro avg)\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
    "    fig.savefig(out_dir / f\"{out_prefix}_roc.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "    # PR\n",
    "    precision, recall, ap = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(Y[:, i], probs[:, i])\n",
    "        ap[i] = average_precision_score(Y[:, i], probs[:, i])\n",
    "\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y.ravel(), probs.ravel())\n",
    "    ap[\"micro\"] = average_precision_score(Y, probs, average=\"micro\")\n",
    "\n",
    "    recall_grid = np.linspace(0, 1, 500)\n",
    "    precision_interp = np.zeros_like(recall_grid)\n",
    "    for i in range(n_classes):\n",
    "        r = recall[i]; p = precision[i]\n",
    "        order = np.argsort(r)\n",
    "        precision_interp += np.interp(recall_grid, r[order], p[order], left=1.0, right=0.0)\n",
    "    precision_interp /= n_classes\n",
    "    ap[\"macro\"] = np.trapz(precision_interp, recall_grid)\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"], label=f\"micro-average PR (AP={ap['micro']:.3f})\")\n",
    "    plt.plot(recall_grid, precision_interp, label=f\"macro-average PR (AP={ap['macro']:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precisionâ€“Recall Curves (macro & micro avg)\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
    "    fig.savefig(out_dir / f\"{out_prefix}_pr.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "    (out_dir / f\"{out_prefix}_roc_pr.json\").write_text(\n",
    "        json.dumps({\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "            \"ap_micro\": float(ap[\"micro\"])\n",
    "        }, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def make_loader(enc, labels, idxs, tokenizer, batch_size, num_workers, sampler_kind=\"weighted\"):\n",
    "    enc_sel = {k: [v[i] for i in idxs] for k, v in enc.items()}\n",
    "    y_sel = np.array(labels)[idxs]\n",
    "    ds = TextClsDataset(enc_sel, y_sel)\n",
    "    collator = DataCollatorWithPadding(tokenizer)\n",
    "    if sampler_kind == \"weighted\":\n",
    "        counts = np.bincount(y_sel, minlength=int(y_sel.max())+1).astype(np.float64)\n",
    "        counts[counts == 0] = 1.0\n",
    "        sample_weights = 1.0 / counts[y_sel]\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "            num_samples=len(y_sel), replacement=True\n",
    "        )\n",
    "        return ds, DataLoader(ds, batch_size=batch_size, sampler=sampler,\n",
    "                              collate_fn=collator, num_workers=num_workers,\n",
    "                              pin_memory=False, persistent_workers=False)\n",
    "    else:\n",
    "        return ds, DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                              collate_fn=collator, num_workers=num_workers,\n",
    "                              pin_memory=False, persistent_workers=False)\n",
    "\n",
    "def quick_train_val_once(model, device, train_loader, val_loader, num_classes, steps, lr, weight_decay, warmup_ratio, grad_clip):\n",
    "    # trainable params only\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    grouped = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(grouped, lr=lr)\n",
    "    total_steps = steps\n",
    "    warmup_steps = int(warmup_ratio * total_steps) if total_steps > 0 else 0\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps or 1)\n",
    "\n",
    "    # class weights from this subset\n",
    "    y_subset = np.concatenate([b[\"labels\"].numpy() for b in train_loader])\n",
    "    class_weights = compute_class_weights(y_subset, num_classes=num_classes).to(device)\n",
    "    focal = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(1):  # one pass (1 epoch) for learning-curve speed\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            loss = focal(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            clip_grad_norm_([p for p in model.parameters() if p.requires_grad], grad_clip)\n",
    "            optimizer.step(); scheduler.step()\n",
    "\n",
    "    # validate macro-F1\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.append(pred.cpu().numpy()); true.append(batch[\"labels\"].cpu().numpy())\n",
    "    preds = np.concatenate(preds); true = np.concatenate(true)\n",
    "\n",
    "    # macro F1\n",
    "    pr_list = []\n",
    "    for c in range(num_classes):\n",
    "        tp = int(((true == c) & (preds == c)).sum())\n",
    "        fp = int(((true != c) & (preds == c)).sum())\n",
    "        fn = int(((true == c) & (preds != c)).sum())\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (2*p*r)/(p+r) if (p+r) > 0 else 0.0\n",
    "        pr_list.append(f1)\n",
    "    return float(np.mean(pr_list)) if pr_list else 0.0\n",
    "\n",
    "def plot_learning_curve_distilbert(tokenizer, enc_train, y_train, enc_val, y_val, class_labels, out_png, out_csv, device):\n",
    "    \"\"\"Stratified subset sizes -> quick 1-epoch finetune -> val macro-F1.\"\"\"\n",
    "    n = len(y_train)\n",
    "    sizes = np.linspace(0.1, 1.0, 8)\n",
    "    num_workers = 0 if os.name == \"nt\" else 2\n",
    "    f1s = []\n",
    "    for frac in sizes:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, train_size=frac, random_state=seed)\n",
    "        (idxs_sub, _) = next(sss.split(np.zeros(n), y_train))\n",
    "        # loaders for train subset + full val\n",
    "        train_ds, train_loader = make_loader(enc_train, y_train, idxs_sub, tokenizer, batch_size, num_workers, \"weighted\")\n",
    "        val_idx = np.arange(len(y_val))\n",
    "        val_ds, val_loader = make_loader(enc_val, y_val, val_idx, tokenizer, batch_size, num_workers, sampler_kind=\"sequential\")\n",
    "\n",
    "        # fresh small model (frozen lower layers)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=len(class_labels)\n",
    "        )\n",
    "        freeze_for_speed(model)\n",
    "        model.to(device)\n",
    "\n",
    "        steps = max(1, len(train_loader))\n",
    "        f1 = quick_train_val_once(model, device, train_loader, val_loader,\n",
    "                                  num_classes=len(class_labels), steps=steps, lr=lr,\n",
    "                                  weight_decay=weight_decay, warmup_ratio=warmup_ratio, grad_clip=grad_clip)\n",
    "        f1s.append(f1)\n",
    "        # cleanup to free RAM\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # plot + csv\n",
    "    fig = plt.figure(figsize=(7, 4.5))\n",
    "    plt.plot((sizes * n).astype(int), f1s, marker=\"o\", label=\"val_macro_f1 (1-epoch)\")\n",
    "    plt.xlabel(\"Training examples (subset)\"); plt.ylabel(\"Macro-F1 (validation)\")\n",
    "    plt.title(\"Learning Curve (quick 1-epoch finetunes)\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
    "    fig.savefig(out_png, dpi=160); plt.close(fig)\n",
    "\n",
    "    pd.DataFrame({\"train_size\": (sizes * n).astype(int), \"val_macro_f1\": f1s}).to_csv(out_csv, index=False)\n",
    "\n",
    "# ----------------- main -----------------\n",
    "\n",
    "def main():\n",
    "    set_seed(seed)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Data\n",
    "    df = pd.read_csv(in_csv)\n",
    "    need = {text_col, label_col}\n",
    "    missing = need - set(df.columns)\n",
    "    if missing:\n",
    "        raise SystemExit(f\"Missing columns: {missing}\")\n",
    "\n",
    "    df = df[[text_col, label_col]].dropna().drop_duplicates()\n",
    "    df[text_col] = df[text_col].apply(clean_text)\n",
    "    df = df[df[text_col].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "    # Label encoding\n",
    "    labels_raw = df[label_col].astype(str).values\n",
    "    classes_sorted = sorted(np.unique(labels_raw).tolist())\n",
    "    label2id = {lbl: i for i, lbl in enumerate(classes_sorted)}\n",
    "    id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "    y = np.array([label2id[s] for s in labels_raw], dtype=np.int64)\n",
    "    num_classes = len(classes_sorted)\n",
    "\n",
    "    (out_dir / \"label_map.json\").write_text(\n",
    "        json.dumps({\"label2id\": label2id, \"id2label\": {int(k): v for k, v in id2label.items()}}, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_stratified(df[text_col].tolist(), y, seed=seed)\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    enc_train = tokenizer(X_train, truncation=True, padding=False, max_length=max_len)\n",
    "    enc_val   = tokenizer(X_val,   truncation=True, padding=False, max_length=max_len)\n",
    "    enc_test  = tokenizer(X_test,  truncation=True, padding=False, max_length=max_len)\n",
    "\n",
    "    train_ds = TextClsDataset(enc_train, y_train)\n",
    "    val_ds   = TextClsDataset(enc_val, y_val)\n",
    "    test_ds  = TextClsDataset(enc_test, y_test)\n",
    "\n",
    "    # DataLoaders\n",
    "    collator = DataCollatorWithPadding(tokenizer)\n",
    "    class_counts = np.bincount(y_train, minlength=num_classes).astype(np.float64)\n",
    "    class_counts[class_counts == 0] = 1.0\n",
    "    sample_weights = 1.0 / class_counts[y_train]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "        num_samples=len(y_train),\n",
    "        replacement=True\n",
    "    )\n",
    "    num_workers = 0 if os.name == \"nt\" else 2\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, sampler=sampler,\n",
    "        collate_fn=collator, num_workers=num_workers,\n",
    "        pin_memory=False, persistent_workers=False\n",
    "    )\n",
    "    val_loader   = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=collator, num_workers=num_workers,\n",
    "        pin_memory=False, persistent_workers=False\n",
    "    )\n",
    "    test_loader  = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=collator, num_workers=num_workers,\n",
    "        pin_memory=False, persistent_workers=False\n",
    "    )\n",
    "\n",
    "    print(f\"CPU threads={torch.get_num_threads()} interop={torch.get_num_interop_threads()}  \"\n",
    "          f\"bsize={batch_size}  max_len={max_len}  workers={num_workers}\")\n",
    "\n",
    "    # ---- Model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_classes, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    model = freeze_for_speed(model)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    grouped = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(grouped, lr=lr)\n",
    "    total_steps = epochs * max(1, len(train_loader))\n",
    "    warmup_steps = int(warmup_ratio * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    # Class weights and FocalLoss\n",
    "    class_weights = compute_class_weights(y_train, num_classes=num_classes).to(device)\n",
    "    focal = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "\n",
    "    # ---- Train loop with early stopping on val macro-F1\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"val_macro_f1\": [], \"val_accuracy\": []}\n",
    "    best_f1, best_state = -1.0, None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            loss = focal(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            clip_grad_norm_([p for p in model.parameters() if p.requires_grad], grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, len(train_loader))\n",
    "\n",
    "        # ---- Validation\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                val_preds.append(pred.cpu().numpy())\n",
    "                val_true.append(batch[\"labels\"].cpu().numpy())\n",
    "        val_preds = np.concatenate(val_preds); val_true = np.concatenate(val_true)\n",
    "\n",
    "        # Metrics\n",
    "        pr_list, rc_list, f1_list = [], [], []\n",
    "        for c in range(num_classes):\n",
    "            tp = int(((val_true == c) & (val_preds == c)).sum())\n",
    "            fp = int(((val_true != c) & (val_preds == c)).sum())\n",
    "            fn = int(((val_true == c) & (val_preds != c)).sum())\n",
    "            p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1 = (2*p*r)/(p+r) if (p+r) > 0 else 0.0\n",
    "            pr_list.append(p); rc_list.append(r); f1_list.append(f1)\n",
    "        val_macro_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "        val_acc = accuracy(val_true, val_preds)\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_macro_f1\"].append(val_macro_f1)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}  val_macro_f1={val_macro_f1:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_macro_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = val_macro_f1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs > early_stop_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Load best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    print(model.config.num_labels)      # should equal your class count\n",
    "    print(model.config.id2label)   \n",
    "    # ---- Test\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            test_preds.append(pred.cpu().numpy())\n",
    "            test_true.append(batch[\"labels\"].cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds); test_true = np.concatenate(test_true)\n",
    "\n",
    "    per_class, macro = per_class_metrics(test_true, test_preds, n_classes=num_classes)\n",
    "    acc = accuracy(test_true, test_preds)\n",
    "    cm = confusion_matrix_counts(test_true, test_preds, n_classes=num_classes)\n",
    "\n",
    "    # ---- Save reports\n",
    "    (out_dir / \"label_map.json\").write_text(\n",
    "        json.dumps({\"label2id\": label2id, \"id2label\": {int(k): v for k, v in id2label.items()}}, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    rep = {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro\": macro,\n",
    "        \"per_class\": {id2label[i]: per_class[i] for i in range(num_classes)}\n",
    "    }\n",
    "    (out_dir / \"test_classification_report.json\").write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    pd.DataFrame(cm, index=[id2label[i] for i in range(num_classes)],\n",
    "                 columns=[id2label[i] for i in range(num_classes)]).to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "    summary = {\n",
    "        \"test_accuracy\": float(acc),\n",
    "        \"test_macro_f1\": float(macro[\"f1\"]),\n",
    "        \"n_train\": int(len(train_ds)), \"n_val\": int(len(val_ds)), \"n_test\": int(len(test_ds)),\n",
    "        \"labels\": [id2label[i] for i in range(num_classes)]\n",
    "    }\n",
    "    (out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "\n",
    "    # ---- Graphs (existing)\n",
    "    if history[\"epoch\"] and len(history[\"epoch\"]) > 0:\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
    "        plt.plot(history[\"epoch\"], history[\"val_macro_f1\"], label=\"val_macro_f1\")\n",
    "        plt.plot(history[\"epoch\"], history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(\"value\"); plt.title(\"Training curves (CPU)\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        fig.savefig(out_dir / \"curves.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1.0)\n",
    "    fig = plt.figure(figsize=(6 + 0.3*num_classes, 5 + 0.3*num_classes))\n",
    "    plt.imshow(cm_norm, aspect=\"auto\")\n",
    "    ticks = np.arange(num_classes)\n",
    "    labels = [id2label[i] for i in ticks]\n",
    "    plt.xticks(ticks, labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, labels)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix (row-normalized)\")\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_dir / \"confusion_matrix.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "    # ---- NEW: Per-class bar chart (test)\n",
    "    plot_per_class_bars(per_class, [id2label[i] for i in range(num_classes)], out_dir / \"per_class_bars.png\")\n",
    "\n",
    "    # ---- NEW: ROC & PR curves (test)\n",
    "    probs, true = collect_probs(model, test_loader, device)\n",
    "    plot_roc_pr_curves(probs, true, [id2label[i] for i in range(num_classes)], out_prefix=\"ovr\")\n",
    "\n",
    "    # ---- NEW: Learning curve (quick 1-epoch runs on stratified subsets)\n",
    "    plot_learning_curve_distilbert(\n",
    "        tokenizer=tokenizer,\n",
    "        enc_train=enc_train, y_train=y_train,\n",
    "        enc_val=enc_val,     y_val=y_val,\n",
    "        class_labels=[id2label[i] for i in range(num_classes)],\n",
    "        out_png=out_dir / \"learning_curve.png\",\n",
    "        out_csv=out_dir / \"learning_curve.csv\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # ---- Save model & tokenizer\n",
    "    model.save_pretrained(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
